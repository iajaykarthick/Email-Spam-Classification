{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/visualization/metrics_visualizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/visualization/metrics_visualizer.py\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class MetricsVisualizer:\n",
    "    def __init__(self, metrics_objects):\n",
    "        \"\"\"\n",
    "        Initializes the visualizer with one or more ClassificationMetrics objects.\n",
    "        \n",
    "        :param metrics_objects: A single ClassificationMetrics object or a list of them.\n",
    "        \"\"\"\n",
    "        self.metrics_objects = metrics_objects if isinstance(metrics_objects, list) else [metrics_objects]\n",
    "\n",
    "    def plot_roc_curve(self):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for metrics in self.metrics_objects:\n",
    "            fpr, tpr = metrics._roc_curve()\n",
    "            roc_auc = metrics._auc()\n",
    "            label = f'{metrics.exp_id} AUC = {roc_auc}' if metrics.exp_id else f'AUC = {roc_auc}'\n",
    "            plt.plot(fpr, tpr, label=label)\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_precision_recall_curve(self):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for metrics in self.metrics_objects:\n",
    "            precision, recall = metrics._precision_recall_curve()\n",
    "            ap = metrics._average_precision()\n",
    "            label = f'{metrics.exp_id} AP = {ap}' if metrics.exp_id else f'AP = {ap}'\n",
    "            plt.plot(recall, precision, label=label)\n",
    "        \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_accuracy_comparison(self):\n",
    "        accuracies = [metrics._accuracy() for metrics in self.metrics_objects]\n",
    "        labels = [metrics.exp_id if metrics.exp_id else \"Unknown\" for metrics in self.metrics_objects]\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.bar(range(len(accuracies)), accuracies, tick_label=labels)\n",
    "        \n",
    "        plt.xlabel('Experiment ID')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Accuracy Comparison')\n",
    "        plt.ylim(0, 1.05)  # Extend y-axis to make room for labels if needed\n",
    "        plt.xticks(rotation=45)  # Rotate labels if they overlap\n",
    "        \n",
    "        # Adding the text labels on the bars\n",
    "        for i, acc in enumerate(accuracies):\n",
    "            plt.text(i, acc + 0.01, f'{acc:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_confusion_matrix(self):\n",
    "        for metrics in self.metrics_objects:\n",
    "            cm = metrics.confusion_matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "            plt.xlabel('Predicted labels')\n",
    "            plt.ylabel('True labels')\n",
    "            plt.title(f'Confusion Matrix for {metrics.exp_id if metrics.exp_id else \"Unknown\"}')\n",
    "            plt.show()\n",
    "            \n",
    "    def plot_f1_scores_comparison(self):\n",
    "        f1_scores = [metrics._f1() for metrics in self.metrics_objects]\n",
    "        labels = [metrics.exp_id if metrics.exp_id else \"Unknown\" for metrics in self.metrics_objects]\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.bar(range(len(f1_scores)), f1_scores, tick_label=labels)\n",
    "\n",
    "        plt.xlabel('Experiment ID')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score Comparison')\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        for i, score in enumerate(f1_scores):\n",
    "            plt.text(i, score + 0.01, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_feature_importance(self, feature_names):\n",
    "        for metrics in self.metrics_objects:\n",
    "            if hasattr(metrics.model, 'feature_importances_'):\n",
    "                importances = metrics.model.feature_importances_\n",
    "                indices = np.argsort(importances)\n",
    "\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                plt.title(f'Feature Importances for {metrics.exp_id if metrics.exp_id else \"Unknown\"}')\n",
    "                plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "                plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
    "                plt.xlabel('Relative Importance')\n",
    "                plt.show()\n",
    "            else:\n",
    "                print(f\"{metrics.exp_id if metrics.exp_id else 'Unknown'} model does not support feature importance.\")\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
