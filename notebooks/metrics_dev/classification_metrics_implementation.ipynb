{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/evaluation/classification_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/evaluation/classification_metrics.py\n",
    "import numpy as np\n",
    "\n",
    "class ClassificationMetrics:\n",
    "    def __init__(self, y_true, y_pred, round_digits=None, exp_id=None):\n",
    "        self.y_true = y_true\n",
    "        self.y_pred = y_pred\n",
    "        self.exp_id = exp_id\n",
    "        self.round_digits = round_digits\n",
    "        \n",
    "    def _round(self, value):\n",
    "        if self.round_digits is not None:\n",
    "            return round(value, self.round_digits)\n",
    "        return value\n",
    "\n",
    "    def _accuracy(self):\n",
    "        return self._round(np.mean(self.y_true == self.y_pred))\n",
    "\n",
    "    def _precision(self):\n",
    "        tp = np.sum((self.y_true == 1) & (self.y_pred == 1))\n",
    "        fp = np.sum((self.y_true == 0) & (self.y_pred == 1))\n",
    "        return self._round(0 if tp + fp == 0 else tp / (tp + fp))\n",
    "\n",
    "    def _recall(self):\n",
    "        tp = np.sum((self.y_true == 1) & (self.y_pred == 1))\n",
    "        fn = np.sum((self.y_true == 1) & (self.y_pred == 0))\n",
    "        return self._round(0 if tp + fn == 0 else tp / (tp + fn))\n",
    "\n",
    "    def _f1(self):\n",
    "        p = self._precision()\n",
    "        r = self._recall()\n",
    "        return self._round(0 if p + r == 0 else 2 * p * r / (p + r))\n",
    "\n",
    "    def _confusion_matrix(self):\n",
    "        tp = np.sum((self.y_true == 1) & (self.y_pred == 1))\n",
    "        tn = np.sum((self.y_true == 0) & (self.y_pred == 0))\n",
    "        fp = np.sum((self.y_true == 0) & (self.y_pred == 1))\n",
    "        fn = np.sum((self.y_true == 1) & (self.y_pred == 0))\n",
    "        return np.array([[tn, fp], [fn, tp]])  # Not rounded, as it contains counts\n",
    "\n",
    "    def _classification_report(self):\n",
    "        return {\n",
    "            'precision': self._precision(),\n",
    "            'recall': self._recall(),\n",
    "            'f1-score': self._f1(),\n",
    "            'support': len(self.y_true)  # Not rounded, as it is a count\n",
    "        }\n",
    "\n",
    "    def _roc_curve(self):\n",
    "        tpr = self._recall()\n",
    "        fp = np.sum((self.y_true == 0) & (self.y_pred == 1))\n",
    "        tn = np.sum((self.y_true == 0) & (self.y_pred == 0))\n",
    "        fpr = self._round(fp / (fp + tn) if fp + tn > 0 else 0)\n",
    "        return fpr, self._round(tpr)\n",
    "\n",
    "    def _auc(self):\n",
    "        fpr, tpr = self._roc_curve()\n",
    "        return self._round(np.trapz([tpr], [fpr]))\n",
    "\n",
    "    def _precision_recall_curve(self):\n",
    "        return self._round(self._recall()), self._round(self._precision())\n",
    "\n",
    "    def _average_precision(self):\n",
    "        r, p = self._precision_recall_curve()\n",
    "        return self._round(np.trapz([p], [r]))\n",
    "\n",
    "    def _calculate_metrics(self):\n",
    "        self.accuracy = self._accuracy()\n",
    "        self.precision = self._precision()\n",
    "        self.recall = self._recall()\n",
    "        self.f1 = self._f1()\n",
    "        self.confusion_matrix = self._confusion_matrix()\n",
    "        self.classification_report = self._classification_report()\n",
    "        self.roc_curve = self._roc_curve()\n",
    "        self.auc = self._auc()\n",
    "        self.precision_recall_curve = self._precision_recall_curve()\n",
    "        self.average_precision = self._average_precision()\n",
    "    \n",
    "    def evaluate(self, return_metrics=False):\n",
    "        self._calculate_metrics()\n",
    "        if return_metrics:\n",
    "            return {\n",
    "                'accuracy': self.accuracy,\n",
    "                'precision': self.precision,\n",
    "                'recall': self.recall,\n",
    "                'f1': self.f1,\n",
    "                'confusion_matrix': self.confusion_matrix,\n",
    "                'classification_report': self.classification_report,\n",
    "                'roc_curve': self.roc_curve,\n",
    "                'auc': self.auc,\n",
    "                'precision_recall_curve': self.precision_recall_curve,\n",
    "                'average_precision': self.average_precision\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.evaluation.classification_metrics import ClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.62, 'precision': 0.6, 'recall': 0.75, 'f1': 0.67, 'confusion_matrix': array([[2, 2],\n",
      "       [1, 3]]), 'classification_report': {'precision': 0.6, 'recall': 0.75, 'f1-score': 0.67, 'support': 8}, 'roc_curve': (0.5, 0.75), 'auc': 0.0, 'precision_recall_curve': (0.75, 0.6), 'average_precision': 0.0}\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0])\n",
    "y_pred = np.array([1, 1, 1, 0, 0, 1, 0, 1])\n",
    "metrics = ClassificationMetrics(y_true, y_pred, round_digits=2)\n",
    "results = metrics.evaluate(return_metrics=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
