{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Algorithm\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting algorithm. The algorithm has gained popularity in machine learning competitions for its performance and speed. Here's a high-level overview of how XGBoost works and its key features:\n",
    "\n",
    "## How XGBoost Works\n",
    "\n",
    "1. **Ensemble of Trees**: XGBoost builds an ensemble of decision trees in a sequential manner. Each tree tries to correct the mistakes of the previous ones.\n",
    "\n",
    "2. **Gradient Boosting**: At its core, XGBoost utilizes the concept of gradient boosting where it constructs new trees that predict the residuals or errors of prior trees combined together in an additive manner.\n",
    "\n",
    "3. **Regularization**: Unlike traditional gradient boosting, XGBoost includes a regularization term (L1 and L2 regularization) on the tree weights, which helps in reducing overfitting.\n",
    "\n",
    "4. **Handling Missing Values**: XGBoost can automatically handle missing values. When it encounters a missing value during a split, it will try both directions and choose the direction that gives it a better split.\n",
    "\n",
    "5. **Tree Pruning**: XGBoost uses depth-first approach and prunes trees backward, a method known as \"pruning\". It grows the tree up to a max depth and then starts pruning it back until the improvement in loss function is below a certain threshold.\n",
    "\n",
    "6. **Learning Rate (Shrinkage)**: Like other boosting methods, XGBoost uses a learning rate to control how quickly it corrects errors. This can prevent overfitting by making the model more robust.\n",
    "\n",
    "7. **Parallel Processing**: XGBoost is designed to be efficient and can run on single machines as well as distributed environments. It parallelizes the construction of trees across multiple CPU cores during the training phase.\n",
    "\n",
    "8. **Objective Function**: The objective function in XGBoost is composed of a loss function (dependent on the problem type) and a regularization term. The algorithm supports custom objective functions as well.\n",
    "\n",
    "9. **Cross-validation**: XGBoost has an in-built routine for cross-validation at each iteration of the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified XGBoost Classifier Class\n",
    "\n",
    "The `SimplifiedXGBoostClassifier` class that is implemented below is a basic representation aiming to capture the essence of how XGBoost operates, specifically focusing on binary classification tasks. Here are the functionalities it includes:\n",
    "\n",
    "- **Binary Classification Support**: It is designed to handle binary classification tasks, such as email spam detection.\n",
    "\n",
    "- **Sequential Tree Building**: The class builds decision trees sequentially, where each tree learns from the mistakes (residuals) of all trees before it.\n",
    "\n",
    "- **Learning Rate**: Incorporates a learning rate to scale the contribution of each tree.\n",
    "\n",
    "- **Logistic Loss for Pseudo-Residuals**: Uses the logistic function to calculate pseudo-residuals for binary classification, facilitating the learning from errors in a probabilistic context.\n",
    "\n",
    "- **Predictions and Probabilities**: It can output class labels for predictions and also provide the probability scores for belonging to the positive class.\n",
    "\n",
    "- **Custom CART Tree Usage**: Utilizes a custom CART implementation for tree building, allowing flexibility in modifying the tree construction process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/models/xgboost.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/xgboost.py\n",
    "import numpy as np\n",
    "from src.models.cart import CART\n",
    "\n",
    "class SimplifiedXGBoostClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "        self.initial_prediction = 0.0  # Initial prediction will be updated to log(odds)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _log_odds(self, p):\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert labels to {0, 1}\n",
    "        y = (y == 1).astype(int)\n",
    "        \n",
    "        # Start with an initial prediction of log(odds)\n",
    "        p = np.mean(y)\n",
    "        self.initial_prediction = self._log_odds(p)\n",
    "        F_m = np.full(len(y), self.initial_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute pseudo-residuals as gradient of logistic loss\n",
    "            preds = self._sigmoid(F_m)\n",
    "            residuals = y - preds\n",
    "            \n",
    "            # Fit a CART to the pseudo-residuals\n",
    "            tree = CART(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion='mse')\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update model predictions\n",
    "            update_preds = tree.predict(X)\n",
    "            F_m += self.learning_rate * update_preds\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        # Aggregate predictions from all trees\n",
    "        F_m = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            F_m += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = self._sigmoid(F_m)\n",
    "        return np.vstack((1 - probs, probs)).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        # Convert probabilities to class labels\n",
    "        return (proba[:, 1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.data.load_dataset import load_spambase\n",
    "from src.models.xgboost import SimplifiedXGBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2760, 57), (920, 57), (921, 57))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_spambase()\n",
    "# Split the dataset into training+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimplifiedXGBoostClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_val)\n",
    "print(f'Accuracy: {np.mean(y_val == y_pred):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
