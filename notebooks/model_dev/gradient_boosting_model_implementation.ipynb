{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Algorithm\n",
    "\n",
    "Gradient Boosting is a powerful machine learning technique that builds on the idea of boosting, where weak learners (typically decision trees) are combined to create a strong predictive model. Here's how the Gradient Boosting algorithm works and its key concepts:\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Ensemble of Weak Learners**: Gradient Boosting involves sequentially adding weak learners (usually decision trees) to an ensemble, where each successive tree corrects errors made by the previous trees.\n",
    "\n",
    "2. **Gradient Descent**: The algorithm uses gradient descent on the loss function to determine how to improve the model with each additional tree. This involves calculating the negative gradient of the loss function (pseudo-residuals) and fitting the next tree to these residuals.\n",
    "\n",
    "3. **Loss Function**: A loss function measures how far the model's predictions are from the actual target values. Common loss functions include mean squared error for regression tasks and logistic loss for classification tasks.\n",
    "\n",
    "4. **Learning Rate**: A learning rate (also known as shrinkage) scales the contribution of each tree. A smaller learning rate requires more trees in the model but often results in better generalization.\n",
    "\n",
    "## Algorithm Steps\n",
    "\n",
    "1. **Initial Model**: Start with a simple model that makes constant predictions for all instances. This could be the mean of the target variable for regression or the log odds of the positive class for binary classification.\n",
    "\n",
    "2. **Compute Pseudo-Residuals**: For each instance, calculate the negative gradient of the loss function with respect to the model's prediction. These pseudo-residuals represent the direction in which to adjust the model's predictions to reduce the loss.\n",
    "\n",
    "3. **Fit a Tree to Pseudo-Residuals**: Train a decision tree to predict the pseudo-residuals from the previous step. This tree aims to correct the errors made by the existing model.\n",
    "\n",
    "4. **Update the Model**: Add the predictions from the new tree to the existing model's predictions, scaled by the learning rate. This updated model represents the current state of the ensemble.\n",
    "\n",
    "5. **Iterate**: Repeat steps 2-4 until a specified number of trees have been added or the improvement in the loss function falls below a threshold.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- **Flexibility**: Can be used for regression, classification, and ranking tasks.\n",
    "- **Handling of Various Types of Data**: Effectively handles numerical and categorical features.\n",
    "- **Automatic Handling of Missing Values**: Trees can learn how to handle missing data implicitly.\n",
    "- **Regularization**: Incorporates techniques to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/models/gradient_boosting.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/gradient_boosting.py\n",
    "import numpy as np\n",
    "from src.models.cart import CART\n",
    "\n",
    "from src.evaluation.classification_metrics import ClassificationMetrics\n",
    "\n",
    "class GradientBoostingBinaryClassifier:\n",
    "    def __init__(self, n_estimators, learning_rate=0.1, max_depth=3, min_samples_split=2):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _log_odds(self, p):\n",
    "        return np.log(p / (1 - p))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions to the mean of the target\n",
    "        p = np.mean(y)\n",
    "        self.initial_prediction = self._log_odds(p)\n",
    "        Fm = np.full(y.shape, self.initial_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate pseudo-residuals\n",
    "            p = self._sigmoid(Fm)\n",
    "            residuals = y - p\n",
    "            \n",
    "            # Fit a tree to the pseudo-residuals\n",
    "            tree = CART(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion='mse')\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update the model with the predictions of the new tree\n",
    "            update = tree.predict(X)\n",
    "            Fm += self.learning_rate * update\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        # Calculate initial predictions\n",
    "        Fm = np.full(X.shape[0], self.initial_prediction)\n",
    "        \n",
    "        # Add the predictions from each tree\n",
    "        for tree in self.trees:\n",
    "            update = tree.predict(X)\n",
    "            Fm += self.learning_rate * update\n",
    "            \n",
    "        # Calculate probabilities using the sigmoid function\n",
    "        probs = self._sigmoid(Fm)\n",
    "        return np.vstack((1 - probs, probs)).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict class labels for samples in X\n",
    "        proba = self.predict_proba(X)\n",
    "        return np.argmax(proba, axis=1)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        # Evaluate the model using the given data\n",
    "        y_pred = self.predict(X)\n",
    "        metrics = ClassificationMetrics(y, y_pred)\n",
    "        return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.data.load_dataset import load_spambase\n",
    "from src.models.gradient_boosting import GradientBoostingBinaryClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2760, 57), (920, 57), (921, 57))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_spambase()\n",
    "\n",
    "# Split the dataset into training+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingBinaryClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, min_samples_split=2)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<src.evaluation.classification_metrics.ClassificationMetrics at 0x156043ee0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
