{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Algorithm\n",
    "\n",
    "Random Forest is an ensemble learning method predominantly used for classification and regression tasks. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean/average prediction (regression) of the individual trees. Random Forests correct for decision trees' habit of overfitting to their training set.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "- **Ensemble Learning**: Technique of combining the predictions from multiple machine learning algorithms to make more accurate predictions than any individual model.\n",
    "- **Decision Tree**: A decision support tool that uses a tree-like model of decisions and their possible consequences. It's the building block of a Random Forest.\n",
    "\n",
    "## How Random Forest Works\n",
    "\n",
    "1. **Bootstrap Aggregating (Bagging)**: Random Forest uses the bagging model and aggregates predictions by voting for the most popular output class or averaging in the case of regression.\n",
    "   \n",
    "2. **Feature Randomness**: When growing each tree, each time a split is considered, a random sample of `m` features is chosen as split candidates from the full set of `p` features. The split only considers these `m` features rather than the best split among all features. This process is known as the random subspace method.\n",
    "   \n",
    "   - Typically, for a classification problem with `p` features, `sqrt(p)` (rounded down) features are used in each split.\n",
    "   - For a regression problem, `p/3` features are used in each split, with a minimum node size of 5 as the default.\n",
    "\n",
    "## Equations\n",
    "\n",
    "- **Gini Impurity** (used to calculate the quality of a split):\n",
    "  \n",
    "  $$ Gini = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "\n",
    "  Where `p_i` is the ratio of class `i` instances among the training instances in the dataset.\n",
    "\n",
    "- **Entropy** (alternative to Gini Impurity):\n",
    "  \n",
    "  $$ Entropy = - \\sum_{i=1}^{n} p_i \\log_2(p_i) $$\n",
    "\n",
    "- **Information Gain** (used to decide which feature to split on):\n",
    "\n",
    "  $$ IG(D, f) = Entropy(D) - \\sum_{v \\in Values(f)} \\frac{|D_v|}{|D|} Entropy(D_v) $$\n",
    "\n",
    "  Where:\n",
    "  - `D` is the dataset,\n",
    "  - `f` is the feature to split on,\n",
    "  - `Values(f)` is the set of all possible values for feature `f`,\n",
    "  - `D_v` is the subset of `D` that has value `v` for feature `f`.\n",
    "\n",
    "## Advantages\n",
    "\n",
    "- It can handle thousands of input variables without variable deletion.\n",
    "- It provides a higher level of accuracy.\n",
    "- Runs efficiently on large datasets.\n",
    "- It can handle missing values and maintains accuracy when a large proportion of the data are missing.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "- Random Forest models are not all that interpretable; they are like black boxes.\n",
    "- For very large data sets, the size of the trees can take up a lot of memory.\n",
    "- While **Random Forest** is less prone to overfitting compared to individual decision trees, it is not entirely immune to this issue, especially in certain situations:\n",
    "\n",
    "  - **With extremely noisy data**: If the data contains a lot of noise, Random Forest can still overfit by capturing the noise in the training data across its many trees, especially if the number of trees (`n_estimators`) is very high.\n",
    "\n",
    "  - **Lack of tree diversity**: If the random subsets of features and data points used to train individual trees are not diverse enough, the ensemble might still overfit by being too tailored to the training data.\n",
    "\n",
    "  - **Complex models with insufficient regularization**: Without proper tuning of the model's hyperparameters (such as `max_depth` for tree depth, `min_samples_split` for the minimum number of samples required to split an internal node, etc.), a Random Forest can grow overly complex trees, which may lead to overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../../src/models/random_forest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/random_forest.py\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    "from src.models.cart import CART\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_features='sqrt', max_depth=None, \n",
    "                 min_samples_split=2, min_impurity_decrease=0, bootstrap=True):\n",
    "        self.n_estimators = n_estimators  # Number of trees\n",
    "        self.max_features = max_features  # The number of features to consider when looking for the best split\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.bootstrap = bootstrap  # Whether bootstrap samples are used when building trees\n",
    "        self.trees = []  # List to store all fitted tree models\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        \n",
    "        # Parallelize tree fitting\n",
    "        self.trees = Parallel(n_jobs=-1)(delayed(self._fit_tree)(X, y) for _ in range(self.n_estimators))\n",
    "        \n",
    "            \n",
    "    def _fit_tree(self, X, y):\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        # Bootstrap sampling\n",
    "        if self.bootstrap:\n",
    "            indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "            X_sample, y_sample = X[indices], y[indices]\n",
    "        else:\n",
    "            X_sample, y_sample = X, y\n",
    "        \n",
    "        # Handle max_features\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_features = int(np.log2(n_features))\n",
    "        elif isinstance(self.max_features, float):\n",
    "            max_features = int(self.max_features * n_features)\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        else:\n",
    "            max_features = n_features\n",
    "        \n",
    "        # Initialize and fit a tree model\n",
    "        tree = CART(max_depth=self.max_depth, \n",
    "                    min_samples_split=self.min_samples_split,\n",
    "                    min_impurity_decrease=self.min_impurity_decrease,\n",
    "                    max_features=max_features)\n",
    "        tree.fit(X_sample, y_sample)\n",
    "        return tree\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Collect predictions from each tree\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # For classification, take the mode (most common label) across trees\n",
    "        mode_preds, _ = mode(tree_preds, axis=0)\n",
    "        return mode_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.data.load_dataset import load_spambase\n",
    "from src.models.random_forest import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2760, 57), (920, 57), (921, 57))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_spambase()\n",
    "# Split the dataset into training+validation and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_features=42, max_depth=10, min_samples_split=5, min_impurity_decrease=0, bootstrap=True)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict(X_val)\n",
    "accuracy = np.mean(y_pred == y_val)\n",
    "print(f'Validation accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
