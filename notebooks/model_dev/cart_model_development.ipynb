{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "\n",
    "CART works by repeatedly partitioning the data into subsets based on the feature that results in \n",
    "the highest information gain (IG) or the lowest Gini impurity for classification, \n",
    "and the lowest mean squared error (MSE) or mean absolute error (MAE) for regression. \n",
    "This process is recursively applied to each subset until a stopping criterion is met \n",
    "(e.g., maximum depth of the tree, minimum samples in a node, or no further improvement).\n",
    "\n",
    "### For Classification\n",
    "\n",
    "- **Gini Impurity**: \n",
    "    A measure of how often a randomly chosen element from the set would be incorrectly labeled \n",
    "    if it was randomly labeled according to the distribution of labels in the subset.\n",
    "    The Gini impurity of a dataset is:\n",
    "\n",
    "    $$ Gini = 1 - \\sum_{i=1}^{n} p_i^2 $$\n",
    "\n",
    "    where $p_i$ is the proportion of items labeled with class $i$ in the dataset.\n",
    "\n",
    "- **Information Gain**: The change in entropy after the dataset is split on an attribute. It's used to decide which feature to split on at each step in building the tree.\n",
    "\n",
    "    $$ IG(D, a) = Entropy(D) - \\sum_{v \\in Values(a)} \\frac{|D_v|}{|D|} Entropy(D_v) $$\n",
    "\n",
    "    where $Entropy(D)$ is the entropy of the dataset $D$, $Values(a)$ are the unique values of attribute $a$, and $D_v$ is the subset of $D$ for which attribute $a$ has value $v$.\n",
    "    \n",
    "### Pre-pruning\n",
    "- `max_depth`: Stop tree growth after reaching a specified depth.\n",
    "\n",
    "- `min_samples_split`: Don't split nodes if fewer than a set number of samples are present.\n",
    "\n",
    "- `min_impurity_decrease`: Only split nodes if a minimum impurity reduction is achieved.\n",
    "\n",
    "- `max_features`: Considers only a random subset of features at each split (similar to Random Forests). This introduces more randomness and helps in diversity and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../../src/models/cart.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../../src/models/cart.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CART:\n",
    "    class Node:\n",
    "        def __init__(self, feature=None, threshold=None, left=None, right=None, num_samples=None, class_distribution=None, value=None):\n",
    "            self.feature = feature # feature index to split\n",
    "            self.threshold = threshold # threshold to split\n",
    "            self.left = left # left node\n",
    "            self.right = right # right node\n",
    "            self.num_samples = num_samples # number of samples in the node\n",
    "            self.class_distribution = class_distribution # class distribution of samples in the node\n",
    "            self.value = value # value of the node if it is a leaf node\n",
    "            \n",
    "        def is_leaf(self):\n",
    "            return self.value is not None\n",
    "        \n",
    "        def __repr__(self):\n",
    "            if self.is_leaf():\n",
    "                return f\"Leaf: {self.value}\"\n",
    "            return f\"Node: feature={self.feature}, threshold={self.threshold}\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_impurity_decrease=0, max_features=None, criterion='gini'):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.root = None\n",
    "        \n",
    "        # for progress tracking\n",
    "        self.max_reached_depth = 0\n",
    "        \n",
    "    def fit(self, X, y, verbose=False):\n",
    "        self.max_reached_depth = 0\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "        if verbose:\n",
    "            print(f\"Maximum depth reached during fit: {self.max_reached_depth}\")\n",
    "        \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        if depth > self.max_reached_depth:\n",
    "            self.max_reached_depth = depth\n",
    "        \n",
    "        # check if stopping criteria are not met\n",
    "        if (self.max_depth is None or depth < self.max_depth) and num_samples >= self.min_samples_split:\n",
    "            # find the best split\n",
    "            best_split = self._best_split(X, y, num_samples, num_features)  \n",
    "            # if the gain is greater than the minimum impurity decrease\n",
    "            if best_split.get('gain', -1) >= self.min_impurity_decrease:\n",
    "                if len(y[best_split['left_indices']]) == 0 or len(y[best_split['right_indices']]) == 0:\n",
    "                    raise ValueError(\"Left or right indices are empty. This should not happen.\")\n",
    "                left_node = self._build_tree(X[best_split['left_indices']], y[best_split['left_indices']], depth + 1)\n",
    "                right_node = self._build_tree(X[best_split['right_indices']], y[best_split['right_indices']], depth + 1)\n",
    "                current_node = self.Node(\n",
    "                    feature=best_split['feature_index'],\n",
    "                    threshold=best_split['threshold'],\n",
    "                    left=left_node,\n",
    "                    right=right_node,\n",
    "                    num_samples=num_samples,\n",
    "                    class_distribution=np.bincount(y)\n",
    "                )\n",
    "                return current_node\n",
    "        \n",
    "        # leaf node\n",
    "        if len(y) == 0:\n",
    "            raise ValueError(\"No samples in the node. This should not happen.\")\n",
    "        leaf_value = self._to_leaf(y)\n",
    "        return self.Node(num_samples=num_samples, class_distribution=np.bincount(y), value=leaf_value)\n",
    "        \n",
    "    def _to_leaf(self, y):\n",
    "        # majority class \n",
    "        # y should be non-negative integer labels\n",
    "        return np.bincount(y).argmax()\n",
    "    \n",
    "    def _best_split(self, X, y, num_samples, num_features):\n",
    "        best_split = {}\n",
    "        max_gain = -float('inf')\n",
    "        \n",
    "        if self.max_features is not None:\n",
    "            num_features_to_sample = min(self.max_features, num_features) # select a subset of features to split\n",
    "            possible_feature_indices = np.random.choice(num_features, num_features_to_sample, replace=False)\n",
    "        \n",
    "        else:\n",
    "            possible_feature_indices = range(num_features)\n",
    "        \n",
    "        for feature_index in possible_feature_indices:\n",
    "            feature_values = X[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            for threshold in possible_thresholds:\n",
    "                left_indices, right_indices = self._split_data(X, y, feature_index, threshold)\n",
    "                if len(left_indices) > 0 and len(right_indices) > 0:\n",
    "                    left_y = y[left_indices]\n",
    "                    right_y = y[right_indices]\n",
    "                    gain = self._information_gain(y, left_y, right_y)\n",
    "                    if gain > max_gain:\n",
    "                        best_split = {\n",
    "                            'feature_index': feature_index,\n",
    "                            'threshold': threshold,\n",
    "                            'left_indices': left_indices,\n",
    "                            'right_indices': right_indices,\n",
    "                            'gain': gain\n",
    "                        }\n",
    "                        max_gain = gain\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _split_data(self, X, y, feature_index, threshold):\n",
    "        left_indices = np.where(X[:, feature_index] <= threshold)[0]\n",
    "        right_indices = np.where(X[:, feature_index] > threshold)[0]\n",
    "        return left_indices, right_indices\n",
    "    \n",
    "    \n",
    "    def _gini(self, y):\n",
    "        # y should be non-negative integer labels\n",
    "        probabilities = np.bincount(y) / len(y)\n",
    "        gini = 1 - np.sum([p**2 for p in probabilities if p > 0])\n",
    "        return gini\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        # y should be non-negative integer labels\n",
    "        probabilities = np.bincount(y) / len(y)\n",
    "        entropy = np.sum([p * -np.log2(p) for p in probabilities if p > 0])\n",
    "        return entropy\n",
    "    \n",
    "    def _information_gain(self, y, left_child, right_child):\n",
    "        weight_1 = len(left_child) / len(y)\n",
    "        weight_2 = len(right_child) / len(y)\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            parent_impurity = self._gini(y)\n",
    "            left_impurity = self._gini(left_child)\n",
    "            right_impurity = self._gini(right_child)\n",
    "        else:\n",
    "            parent_impurity = self._entropy(y)\n",
    "            left_impurity = self._entropy(left_child)\n",
    "            right_impurity = self._entropy(right_child) \n",
    "        \n",
    "        gain = parent_impurity - (weight_1 * left_impurity + weight_2 * right_impurity)\n",
    "        \n",
    "        return gain\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_input(x, self.root) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def _predict_input(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        feature_value = x[node.feature]\n",
    "        if feature_value <= node.threshold:\n",
    "            return self._predict_input(x, node.left)\n",
    "        else:\n",
    "            return self._predict_input(x, node.right)\n",
    "\n",
    "    def export_graphviz(self, full_verbose=False, leaf_verbose=False):\n",
    "        \n",
    "        # Initialize the unique ID generator\n",
    "        unique_id_generator = self._unique_id_generator()\n",
    "        \n",
    "        # Start the dot string with the graph type\n",
    "        dot_string = \"digraph Tree {\\n size=\\\"10,10\\\"; rankdir=\\\"LR\\\";\\n\"\n",
    "    \n",
    "        # Begin the recursive process starting from the root node\n",
    "        dot_string, root_id = self._export_node(self.root, dot_string, unique_id_generator, full_verbose=full_verbose, leaf_verbose=leaf_verbose)\n",
    "\n",
    "        # Close the graph string\n",
    "        dot_string += \"}\\n\"\n",
    "\n",
    "        return dot_string\n",
    "\n",
    "    def _unique_id_generator(self):\n",
    "        current_id = 0\n",
    "        while True:\n",
    "            yield current_id\n",
    "            current_id += 1\n",
    "\n",
    "    def _export_node(self, node, dot_string, unique_id_generator, full_verbose=False, leaf_verbose=False):\n",
    "        # Get a unique ID for the current node\n",
    "        unique_id = next(unique_id_generator)\n",
    "        if node.is_leaf():\n",
    "            \n",
    "            # Leaf node definition with value\n",
    "            if full_verbose or leaf_verbose:\n",
    "                dot_string += f\"  {unique_id} [shape=box, label=\\\"Predicted class: {node.value} \\\\n samples = {node.num_samples}\\\\n class distribution = {node.class_distribution}\\\"];\\n\"\n",
    "            else:\n",
    "                dot_string += f\"  {unique_id} [shape=box, label=\\\"Predicted class: {node.value}\\\"];\\n\"\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Decision node definition\n",
    "            if full_verbose:\n",
    "                dot_string += f\"  {unique_id} [label=\\\"X[{node.feature}] <= {node.threshold:.3f} \\\\n samples = {node.num_samples}\\\\n class distribution = {node.class_distribution}\\\"];\\n\"\n",
    "            else:\n",
    "                dot_string += f\"  {unique_id} [label=\\\"X[{node.feature}] <= {node.threshold:.3f}\\\"];\\n\"            \n",
    "            \n",
    "            # Recursively process the left child\n",
    "            dot_string, left_child_id = self._export_node(node.left, dot_string, unique_id_generator, full_verbose=full_verbose, leaf_verbose=leaf_verbose)\n",
    "            \n",
    "            # Add edge to the left child\n",
    "            dot_string += f\"  {unique_id} -> {left_child_id} [label=\\\"true\\\"];\\n\"\n",
    "            \n",
    "            # Recursively process the right child\n",
    "            dot_string, right_child_id = self._export_node(node.right, dot_string, unique_id_generator, full_verbose=full_verbose, leaf_verbose=leaf_verbose)\n",
    "            \n",
    "            # Add edge to the right child\n",
    "            dot_string += f\"  {unique_id} -> {right_child_id} [label=\\\"false\\\"];\\n\"\n",
    "\n",
    "        return dot_string, unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.load_dataset import load_spambase\n",
    "from src.models.cart import CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_spambase()\n",
    "\n",
    "tree = CART(max_depth=10, min_samples_split=2, min_impurity_decrease=0, max_features=None, criterion='gini')\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev_reports/cart_tree_custom_implementation.pdf'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "# Get DOT data\n",
    "dot_data = tree.export_graphviz(full_verbose=False, leaf_verbose=True)\n",
    "\n",
    "# Draw graph\n",
    "graph = Source(dot_data, format=\"pdf\", filename=\"cart_tree_custom_implementation\", directory=\"dev_reports\")\n",
    "graph.render(view=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "email_spam_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
